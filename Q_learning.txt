This is the pseudo-code of the algorithm inspired by the "deep Q-learning" algorithm devolped by DeepMind
in the paper "Playing Atari with Deep Reinforcement Learning". I am following their notation.



Initialize memory capacity N of D (memory that stores 4-tuples of the kind(s, a, r, s'))
Initialize epsilon
Initialize discount rate gamma
Initialize the NN Q with random weights THETA

For episode = 1 ... M : 
		Initialize s = s_0 (initial state)
		While (s not terminal state):
				choose action a with epsilon-greedy policy w.r.t. current Q 
				execute a and observe reward r and next state s'
				store (s,a,r,s') in D
				s = s'
				sample a random batch of 4-tuples (s_j, a_j, r_j, s_j') from D 
				set targets y_j as follows

				if (s_j' is terminal state):
						y_j = r_j
				else:
						y_j = r_j + gamma*[max_(a') Q(s_j', a', THETA)]

				perform 1 gradient descent step on (y_j - Q(s_j, a_j, THETA)^2 

				s = s'
end for
